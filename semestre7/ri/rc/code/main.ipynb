{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d33503a-5601-4dcd-9438-60a4e1603c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 19:19:38.588693: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-18 19:19:38.592129: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-18 19:19:38.592141: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-18 19:19:48.019466: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-18 19:19:48.019486: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (joao): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b87e553-765e-4909-8580-4a47fcd52670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopedrosa/miniconda3/lib/python3.9/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_lg' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.4.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    text_tokens = word_tokenize(sentence)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "    \n",
    "    return \" \".join(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfb2888-774b-415a-b5d4-5d5ff46637ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ebe18c-0027-4c14-b5c6-a782467ea6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes StopWords and makes Stemmization\n",
    "def pre_process(string):\n",
    "    string = stemSentence(string)\n",
    "    string = remove_stopwords(string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9de54c2-6dbd-411c-a21b-922e6437037f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"latin omega , omega , addit letter latin alphabet , base lowercas greek letter omega ⟨ω⟩ . wa includ latin letter mann dalbi 1982 revis african refer alphabet ha use public kulango languag côte d'ivoir 1990 . kulango public letter v hook ⟨ʋ⟩ latin upsilon ⟨ʊ⟩ found instead .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee62205-6059-411e-972f-102d8b59de05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4641784it [2:09:31, 597.28it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/corpus.jsonl') as f:\n",
    "    with open('../data/corpus/no_stop_words/corpus_keywords_processed.jsonl', 'w') as f2:\n",
    "        for line in tqdm(f):\n",
    "            data     = json.loads(line)\n",
    "            \n",
    "            # Preprocessing\n",
    "            data[\"title\"]    = pre_process(data[\"title\"])\n",
    "            data[\"text\"]     = pre_process(data[\"text\"])\n",
    "            data[\"keywords\"] = [pre_process(k) for k in data[\"keywords\"]]\n",
    "            \n",
    "            # Storing\n",
    "            new_data = {\"id\": data[\"id\"], \"contents\": data[\"title\"] + \"\\n\" + data[\"text\"] + \"\\n\"}\n",
    "            for k in data[\"keywords\"]:\n",
    "                new_data[\"contents\"] += k + \"\\n\"\n",
    "            n_string = json.dumps(new_data)\n",
    "            f2.write(n_string)\n",
    "            f2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbd4c9fb-aa45-47e8-a7ba-ff2635f2c89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4641784it [00:29, 158276.96it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/corpus/no_stop_words/corpus_keywords_processed.jsonl') as f:\n",
    "    with open('../data/corpus_pyt.jsonl', 'w') as f2:\n",
    "        for line in tqdm(f):\n",
    "            data     = json.loads(line)\n",
    "            new_data = {\"docno\": data[\"id\"], \"text\": data[\"contents\"]}\n",
    "            \n",
    "            n_string = json.dumps(new_data)\n",
    "            f2.write(n_string)\n",
    "            f2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4146d6-efe0-49cc-980b-70fd6e96bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus.jsonl') as f:\n",
    "    with open('../data/corpus/no_stop_words/corpus_keywords_processed.jsonl', 'w') as f2:\n",
    "        for line in tqdm(f):\n",
    "            data     = json.loads(line)\n",
    "            # Storing\n",
    "            new_data = {\"docno\": data[\"id\"], \"contents\": data[\"\"] + \"\\n\" + data[\"text\"] + \"\\n\"}\n",
    "            for k in data[\"keywords\"]:\n",
    "                new_data[\"contents\"] += k + \"\\n\"\n",
    "            n_string = json.dumps(new_data)\n",
    "            f2.write(n_string)\n",
    "            f2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8405634-db9a-42b7-9784-d48b2b992cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're gonna use or Spacy Neural Network to assist us in the ranking\n",
    "\n",
    "def answer(searcher, q):\n",
    "    data = {\"QueryId\":[], \"EntityId\":[]}\n",
    "    # data = {\"QueryId\":[], \"EntityId\":[], \"Score\":[]}hmmm então\n",
    "    for i in range(len(q)):\n",
    "        qidx = q[\"QueryId\"][i]\n",
    "        hits = searcher.search(pre_process(q[\"Query\"][i]), k = 100)\n",
    "        for hit in hits:\n",
    "            data[\"QueryId\"].append(str(qidx).zfill(3))\n",
    "            data[\"EntityId\"].append(hit.docid)\n",
    "            # data[\"Score\"].append(hit.score)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690bb982-821b-4d71-bf4e-c4d1bc1eeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tq = pd.read_csv(\"../data/test_queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e9843a-4368-4be7-b666-27ba32a91a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = LuceneSearcher('../data/indexes/sparse_no_stop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff985ca3-377d-4294-b6ea-904daa7e79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = searcher.search(pre_process(\"2001 debut album\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da702c1-c84b-4e24-9a15-cafed704d3b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912152\n",
      "8.619500160217285\n",
      "{\n",
      "  \"id\" : \"1912152\",\n",
      "  \"contents\" : \"seriou\\nseriou debut studio album american rapper t.i . , releas octob 9 , 2001 arista record . album spawn eponym singl , featur jamaican regga entertain beeni man . hi debut singl , \\\\i seriou , \\\\ wa releas june 26 , 2001 . singl receiv littl airplay fail chart .\\n2001 debut album\\nalbum produc dj toomp\\nalbum produc jazz pha\\nalbum produc lil jon\\nalbum produc neptun\\narista record album\\n\"\n",
      "}\n",
      "0000002\n",
      "8.617500305175781\n",
      "{\n",
      "  \"id\" : \"0000002\",\n",
      "  \"contents\" : \"! ! ! ( album )\\n! ! ! eponym debut album ! ! ! . wa releas 2001 gold standard laboratori .\\n! ! ! album\\n2001 debut album\\nenglish-languag album\\n\"\n",
      "}\n",
      "1894433\n",
      "8.552900314331055\n",
      "{\n",
      "  \"id\" : \"1894433\",\n",
      "  \"contents\" : \"humanist ( album )\\nhumanist debut album abandon pool . wa releas septemb 2001 .\\n2001 debut album\\nextasi record album\\ntommi walter album\\n\"\n",
      "}\n",
      "4151319\n",
      "8.514100074768066\n",
      "{\n",
      "  \"id\" : \"4151319\",\n",
      "  \"contents\" : \"voic ( russel watson album )\\nvoic 2001 debut album british tenor russel watson .\\n2001 debut album\\nclassic crossov album\\n\"\n",
      "}\n",
      "1946329\n",
      "8.494400024414062\n",
      "{\n",
      "  \"id\" : \"1946329\",\n",
      "  \"contents\" : \"search ... ( n.e.r.d album )\\nsearch ... debut album releas n.e.r.d . group origin releas europ 2001 , keli ' neptunes-produc album wanderland , wa better receiv . similarli keli ' earli work , origin version search ...\\n2001 debut album\\n2002 debut album\\nalbum certifi gold record industri associ america\\nalbum produc neptun\\nn.e.r.d album\\n\"\n",
      "}\n",
      "1490608\n",
      "8.486700057983398\n",
      "{\n",
      "  \"id\" : \"1490608\",\n",
      "  \"contents\" : \"forc natur ( tank album )\\nforc natur debut studio album american entertain tank , wa releas march 13 , 2001 blackground record . album debut # 1 r & b/hip-hop album chart & debut # 7 billboard 200 .\\n2001 album\\nalbum certifi gold record industri associ america\\nalbum produc bud'da\\n\"\n",
      "}\n",
      "4029871\n",
      "8.468899726867676\n",
      "{\n",
      "  \"id\" : \"4029871\",\n",
      "  \"contents\" : \"tantric ( album )\\ntantric debut album american post-grung band . wa releas februari 13 , 2001 debut # 193 billboard 200 . album eventu peak # 71 reach platinum statu hit-singl \\\\breakdown , \\\\ follow singl \\\\astounded\\\\ \\\\mourning.\\\\\\n2001 debut album\\nalbum produc tobi wright\\nmaverick record album\\n\"\n",
      "}\n",
      "1110173\n",
      "8.466500282287598\n",
      "{\n",
      "  \"id\" : \"1110173\",\n",
      "  \"contents\" : \"dark day ( load album )\\ndark day debut studio album american hard rock band load . record decemb 2000 februari 2001 jupit studio seattl , washington , wa self-releas juli 2001 .\\n2001 debut album\\nalbum produc martin feveyear\\n\"\n",
      "}\n",
      "0943782\n",
      "8.458499908447266\n",
      "{\n",
      "  \"id\" : \"0943782\",\n",
      "  \"contents\" : \"cinderellen\\ncinderellen 2001 debut solo album canadian ellen reid .\\n2001 debut album\\nellen reid album\\n\"\n",
      "}\n",
      "4264513\n",
      "8.45849895477295\n",
      "{\n",
      "  \"id\" : \"4264513\",\n",
      "  \"contents\" : \"true ( trinityroot album )\\ntrue debut album new zealand band , trinityroot , releas 2001 .\\n2001 debut album\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for hit in h:\n",
    "    print(hit.docid)\n",
    "    print(hit.score)\n",
    "    print(hit.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b82fbe16-c3dd-4bf1-998a-46cf495362f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tq.to_csv(\"../data/sample_queries.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c93bd055-f082-4757-8572-2f1f99367c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans = answer(searcher, tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98daac84-feca-4095-bab9-03b110f8b145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryId</th>\n",
       "      <th>EntityId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>0366601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>3572540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002</td>\n",
       "      <td>0758546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002</td>\n",
       "      <td>2508826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002</td>\n",
       "      <td>1850915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23200</th>\n",
       "      <td>465</td>\n",
       "      <td>3543283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23201</th>\n",
       "      <td>465</td>\n",
       "      <td>1470065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23202</th>\n",
       "      <td>465</td>\n",
       "      <td>4537965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23203</th>\n",
       "      <td>465</td>\n",
       "      <td>0103926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23204</th>\n",
       "      <td>465</td>\n",
       "      <td>3164224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      QueryId EntityId\n",
       "0         002  0366601\n",
       "1         002  3572540\n",
       "2         002  0758546\n",
       "3         002  2508826\n",
       "4         002  1850915\n",
       "...       ...      ...\n",
       "23200     465  3543283\n",
       "23201     465  1470065\n",
       "23202     465  4537965\n",
       "23203     465  0103926\n",
       "23204     465  3164224\n",
       "\n",
       "[23205 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a28d4390-3e55-4f3b-8dca-36f3312e6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv(\"../data/results/20-57.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf97e63e-9681-4291-8131-36afeb4e4589",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 1537119 13.91340\n",
      " 2 3481031 13.83070\n",
      " 3 3357186 13.62200\n",
      " 4 3357207 13.58150\n",
      " 5 2989663 13.52220\n",
      " 6 4463109 13.48630\n",
      " 7 3357185 13.48290\n",
      " 8 1464351 13.40500\n",
      " 9 3357195 13.40500\n",
      "10 3357214 13.40500\n",
      "11 0650179 13.36640\n",
      "12 2552426 13.36640\n",
      "13 3357184 13.36170\n",
      "14 3357216 13.32810\n",
      "15 0989109 13.28990\n",
      "16 2542131 13.28990\n",
      "17 3357206 13.28990\n",
      "18 3373092 13.28990\n",
      "19 3357182 13.25200\n",
      "20 0614032 13.24730\n",
      "21 3357211 13.24730\n",
      "22 0332427 13.20210\n",
      "23 0527265 13.20210\n",
      "24 2398754 13.20210\n",
      "25 2454875 13.20210\n",
      "26 4551472 13.20210\n",
      "27 3131868 13.18400\n",
      "28 3357197 13.17670\n",
      "29 1548007 13.15720\n",
      "30 3357223 13.15720\n",
      "31 0719244 13.07260\n",
      "32 4538071 13.06830\n",
      "33 2735064 13.04510\n",
      "34 3612207 13.04510\n",
      "35 0343050 13.02430\n",
      "36 0964477 13.02430\n",
      "37 3357187 13.02430\n",
      "38 0534861 12.98060\n",
      "39 1741349 12.98060\n",
      "40 3357149 12.98060\n",
      "41 3357164 12.98060\n",
      "42 3357178 12.98060\n",
      "43 3357179 12.97020\n",
      "44 0805047 12.96310\n",
      "45 3357181 12.96310\n",
      "46 3357158 12.93720\n",
      "47 3357188 12.93400\n",
      "48 1973148 12.90900\n",
      "49 2735061 12.89410\n",
      "50 3357198 12.89410\n",
      "51 3400233 12.89410\n",
      "52 3583705 12.89410\n",
      "53 1963831 12.88220\n",
      "54 1930064 12.86230\n",
      "55 2542132 12.86230\n",
      "56 0595784 12.85540\n",
      "57 3357193 12.85540\n",
      "58 3357165 12.85120\n",
      "59 2917579 12.82880\n",
      "60 4498350 12.82880\n",
      "61 1044278 12.82680\n",
      "62 2624802 12.80220\n",
      "63 4406900 12.79150\n",
      "64 0338732 12.77580\n",
      "65 1839302 12.77580\n",
      "66 3357208 12.74950\n",
      "67 0915088 12.72330\n",
      "68 2085655 12.69720\n",
      "69 3357156 12.69720\n",
      "70 0560684 12.64530\n",
      "71 3308885 12.64530\n",
      "72 3357201 12.64530\n",
      "73 0353016 12.59380\n",
      "74 0913130 12.59380\n",
      "75 2901003 12.59380\n",
      "76 3299744 12.59380\n",
      "77 3357191 12.58340\n",
      "78 3357227 12.54940\n",
      "79 3657831 12.54940\n",
      "80 0243357 12.54280\n",
      "81 0650445 12.54280\n",
      "82 2828352 12.54280\n",
      "83 3357205 12.54280\n",
      "84 2735062 12.51550\n",
      "85 3357218 12.51550\n",
      "86 0911604 12.49210\n",
      "87 0986161 12.49210\n",
      "88 1578813 12.49210\n",
      "89 1877180 12.49210\n",
      "90 3357183 12.49210\n",
      "91 3357217 12.49209\n",
      "92 3829324 12.49209\n",
      "93 1400287 12.44840\n",
      "94 3346642 12.44840\n",
      "95 3357196 12.44840\n",
      "96 3966098 12.44190\n",
      "97 3970149 12.44190\n",
      "98 3201975 12.41510\n",
      "99 3357200 12.41510\n",
      "100 0798071 12.39210\n"
     ]
    }
   ],
   "source": [
    "hits = searcher.search('potato potato', k = 100)\n",
    "\n",
    "for i in range(len(hits)):\n",
    "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84c8bc-cfcb-4425-a651-4de81776ca8c",
   "metadata": {},
   "source": [
    "### Let's make a Neural Network 8)\n",
    "First we're gonna build the dataset then do the training with python -m shennaningans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef9dde1f-0223-485f-a712-f8cebbcabac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 8202/8202 [00:17<00:00, 460.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Let's throw stuff into memory. This uses huge memory so I'll run it once then throw only useful information into a CSV\n",
    "# in a way that I can retrieve information later without sending it all to memory.\n",
    "\n",
    "# corpus = {}\n",
    "# with open('../data/corpus.jsonl') as f:\n",
    "#     for line in tqdm(f):\n",
    "#         data               = json.loads(line)\n",
    "#         corpus[data['id']] = data\n",
    "        \n",
    "# with open('../data/corpus.pickle', 'wb') as handle:\n",
    "#     pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Loading\n",
    "with open('../data/corpus.pickle', 'rb') as handle:\n",
    "    corpus = pickle.load(handle)\n",
    "\n",
    "# Reading training queries\n",
    "train_queries = pd.read_csv(\"../data/train_queries.csv\")\n",
    "qdict = {train_queries[\"QueryId\"][i]:train_queries[\"Query\"][i] for i in range(len(train_queries))}\n",
    "\n",
    "train_answers = pd.read_csv(\"../data/train_qrels.csv\")\n",
    "\n",
    "# Building a complete csv\n",
    "train_dict = {\"Text\":[], \"Class\":[]}\n",
    "\n",
    "for i in tqdm(range(len(train_answers))):\n",
    "    data = corpus[str(train_answers['EntityId'][i]).zfill(7)]\n",
    "    train_dict[\"Text\"].append(pre_process(qdict[train_answers[\"QueryId\"][i]]) + \"\\n\" + \\\n",
    "                              pre_process(data[\"title\"]) + \"\\n\" + \\\n",
    "                              pre_process(data[\"text\"]) + \"\\n\" + \\\n",
    "                              \"\\n\".join([pre_process(k) for k in data[\"keywords\"]]))\n",
    "    \n",
    "    train_dict[\"Class\"].append(train_answers[\"Relevance\"][i])\n",
    "    \n",
    "train_df.to_csv(\"../data/train_data.csv\", index = False)\n",
    "train_df = pd.DataFrame.from_dict(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a158372-dd20-4de4-aa98-1bc791322af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae64d5d4-b606-4994-8da8-27515a92e46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's transform data into Spacy docs\n",
    "# nlp is spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "'''\n",
    "Receives Tuples and return Spacy Docs.\n",
    "\n",
    "Input:\n",
    "    data: List of tuples of format (Text, Class)\n",
    "    classes: List or Set of possible classes - Set is Faster\n",
    "    nlp: Spacy model to use. If you don't know what to use, for portuguese documentos you should use \"nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "Output:\n",
    "    A list of spacy Docs\n",
    "'''\n",
    "def make_spacy_docs(data, classes, nlp):\n",
    "    docs = []\n",
    "\n",
    "    # Here the text is already added to a doc with the nlp.pip function\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True)):\n",
    "        if label not in classes:\n",
    "            raise Exception(\"One of your examples in data have a label that is not listed in the possible classes.\")\n",
    "        # Create label for each class\n",
    "        for c in classes:\n",
    "            if(c != label):\n",
    "                doc.cats[c] = 0\n",
    "            else:\n",
    "                doc.cats[c] = 1\n",
    "        # Add document to return list\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4655ca-46f7-48ee-9171-cbb099a203e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europ solar power facil\\nandasol solar power s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>europ solar power facil\\narchimed solar power ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>europ solar power facil\\nbp solar\\nbp solar wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>europ solar power facil\\nbavaria solarpark\\nba...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>europ solar power facil\\nbeeckerwerth\\nbeecker...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8197</th>\n",
       "      <td>member beaux art trio\\ndaniel guilet\\ndaniel g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>member beaux art trio\\nida kavafian\\nida kavaf...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8199</th>\n",
       "      <td>member beaux art trio\\nisidor cohen\\ncompos bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>member beaux art trio\\nmenahem pressler\\nmenah...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>member beaux art trio\\nvictor laloux\\nvictor a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8202 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Class\n",
       "0     europ solar power facil\\nandasol solar power s...      1\n",
       "1     europ solar power facil\\narchimed solar power ...      2\n",
       "2     europ solar power facil\\nbp solar\\nbp solar wa...      1\n",
       "3     europ solar power facil\\nbavaria solarpark\\nba...      2\n",
       "4     europ solar power facil\\nbeeckerwerth\\nbeecker...      2\n",
       "...                                                 ...    ...\n",
       "8197  member beaux art trio\\ndaniel guilet\\ndaniel g...      1\n",
       "8198  member beaux art trio\\nida kavafian\\nida kavaf...      2\n",
       "8199  member beaux art trio\\nisidor cohen\\ncompos bo...      1\n",
       "8200  member beaux art trio\\nmenahem pressler\\nmenah...      2\n",
       "8201  member beaux art trio\\nvictor laloux\\nvictor a...      1\n",
       "\n",
       "[8202 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017a7953-27da-4d97-bfea-fb514fb9f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\"partially_relevant\", \"highly_relevant\"}\n",
    "train_tuples = []\n",
    "for i in range(len(train_df)):\n",
    "    text = train_df[\"Text\"][i]\n",
    "    c    = \"partially_relevant\" if train_df[\"Class\"][i] == 1 else \"highly_relevant\"\n",
    "    train_tuples.append((text, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a32aa269-c977-4588-b13d-47357adb578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6561it [00:37, 174.88it/s]\n",
      "1641it [00:09, 165.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# We're not gonna shuffle to keep independent queries in Validation\n",
    "TRAIN = int(0.8 * len(train_tuples))\n",
    "\n",
    "# TRAIN\n",
    "train_docs = make_spacy_docs(train_tuples[:TRAIN], classes, nlp)\n",
    "doc_bin    = DocBin(docs = train_docs)\n",
    "doc_bin.to_disk(\"../data/spacy/train.spacy\")\n",
    "\n",
    "# VAL\n",
    "val_docs = make_spacy_docs(train_tuples[TRAIN:], classes, nlp)\n",
    "doc_bin  = DocBin(docs = val_docs)\n",
    "doc_bin.to_disk(\"../data/spacy/val.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba540631-c705-4460-acfe-f959fcf106f7",
   "metadata": {},
   "source": [
    "Now we use python -m spacy train to train our model and then we go to next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f609b28-9843-45a1-959e-794b60ce4c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopedrosa/miniconda3/lib/python3.9/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"../data/spacy/models/07-18-tf/model-best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2721d59-e70a-43a2-b315-4632519b01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're gonna use or Spacy Neural Network to assist us in the ranking\n",
    "\n",
    "def answer(searcher, q, nlp):\n",
    "    data = {\"QueryId\":[], \"EntityId\":[]}\n",
    "    # data = {\"QueryId\":[], \"EntityId\":[], \"Score\":[]}\n",
    "    for i in tqdm(range(len(q))):\n",
    "        qidx  = q[\"QueryId\"][i]\n",
    "        qtext = pre_process(q[\"Query\"][i])\n",
    "        hits = searcher.search(qtext, k = 100)\n",
    "        \n",
    "        prel = []\n",
    "        hrel = []\n",
    "        for hit in hits:\n",
    "            # Passing through network\n",
    "            text = qtext+\"\\n\"+ json.loads(hit.raw)[\"contents\"]+\"\\n\"\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            if(doc.cats['highly_relevant'] > 0.5):\n",
    "                hrel.append([str(qidx).zfill(3), hit.docid])\n",
    "            else:\n",
    "                prel.append([str(qidx).zfill(3), hit.docid])\n",
    "        \n",
    "        # Highly Relevant First\n",
    "        for qid, eid in hrel:\n",
    "            data[\"QueryId\"].append(qid)\n",
    "            data[\"EntityId\"].append(eid)\n",
    "        \n",
    "        # Partially Relevant After\n",
    "        for qid, eid in prel:\n",
    "            data[\"QueryId\"].append(qid)\n",
    "            data[\"EntityId\"].append(eid)\n",
    "            \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c7373a2-a8e8-422f-acb4-2e6f23b76ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/233 [00:00<?, ?it/s]/home/joaopedrosa/miniconda3/lib/python3.9/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|█████████████████████████████████████████| 233/233 [42:08<00:00, 10.85s/it]\n"
     ]
    }
   ],
   "source": [
    "df_ans = answer(searcher, tq, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ce1b8e0-3779-4a88-9fb2-edc00c6b1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv(\"../data/results/spacy-transformer_18-35.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ca5c51f-8570-41bc-b0c8-e6ed94288bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same but we order based only in the relevance probability of the network\n",
    "\n",
    "def answer(searcher, q, nlp):\n",
    "    data = {\"QueryId\":[], \"EntityId\":[]}\n",
    "    # data = {\"QueryId\":[], \"EntityId\":[], \"Score\":[]}\n",
    "    for i in tqdm(range(len(q))):\n",
    "        qidx  = q[\"QueryId\"][i]\n",
    "        qtext = pre_process(q[\"Query\"][i])\n",
    "        hits = searcher.search(qtext, k = 100)\n",
    "        \n",
    "        h = []\n",
    "        for hit in hits:\n",
    "            # Passing through network\n",
    "            text = qtext+\"\\n\"+ json.loads(hit.raw)[\"contents\"]+\"\\n\"\n",
    "            doc = nlp(text)\n",
    "            # We use partially_relevant here because it is basically (1 - highly_relevant) and we will sort it by increasing order.\n",
    "            h.append([doc.cats['partially_relevant'], str(qidx).zfill(3), hit.docid])\n",
    "            \n",
    "        h = sorted(h)\n",
    "        \n",
    "        # Ordered by confidence on highly_relevant class\n",
    "        for _, qid, eid in h:\n",
    "            data[\"QueryId\"].append(qid)\n",
    "            data[\"EntityId\"].append(eid)\n",
    "        \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7eb79961-a653-4d1a-aa58-5fc5db7964ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 233/233 [08:55<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "df_ans = answer(searcher, tq, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69105ed1-c927-4605-b73c-8631d81b1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv(\"../data/results/spacy-ordered_16-01.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62362bda-2d8c-4a04-adad-920cef31fefb",
   "metadata": {},
   "source": [
    "## Okay let's try Pyterrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4967c0-8fcd-42c7-8851-7c2826e9481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_file(filename):\n",
    "    import json\n",
    "    with open(filename, 'rt') as file:\n",
    "        for l in file:\n",
    "            # assumes that each line contains 'docno', 'text' attributes\n",
    "            # yields a dictionary for each json line\n",
    "            yield json.loads(l)\n",
    "\n",
    "# indexref4 = pt.IterDictIndexer(\"./index\", meta=['docno', 'text'], meta_lengths=[20, 4096]).index(iter_file(\"/path/to/file.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceccd6c7-6408-4da5-8ea2-ff553b4d6b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89db898e-2572-4b02-b380-21ac79b691c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:52:55.010 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (4641604) - further warnings are suppressed\n",
      "18:52:55.037 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 1 empty documents\n"
     ]
    }
   ],
   "source": [
    "index = pt.IterDictIndexer(\"../data/indexes/pyterrier\", meta={'docno': 20, 'text': 1024}, type=pt.index.IndexingType(1), overwrite=True, verbose=True)\n",
    "index = index.index(iter_file(\"../data/corpus_pyt.jsonl\"), fields = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c2cc85-0ca8-4ae2-9560-c09dcc7fdd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b78e638-1d54-4132-a2c5-10e8ddc7f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78440/106473622.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tq[\"query\"][i] = text\n"
     ]
    }
   ],
   "source": [
    "tq = pd.read_csv(\"../data/test_queries.csv\")\n",
    "tq.columns = [\"qid\", \"query\"]\n",
    "\n",
    "for i in range(len(tq)):\n",
    "    text = tq[\"query\"][i]\n",
    "    text = pre_process(text)\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace(\"!\", \"\")\n",
    "    tq[\"query\"][i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7bb9780-5b2d-4380-adb3-9b846da2e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = bm25.transform(tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a53b7792-7b01-4f7f-be39-6e8a8605a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopedrosa/miniconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# dataset = pt.get_dataset(\"irds:vaswani\")\n",
    "# bm25 = pt.BatchRetrieve(pt.get_dataset(\"vaswani\").get_index(), wmodel=\"BM25\")\n",
    "# mono_pipeline = bm25 >> pt.text.get_text(dataset, \"text\") >> monoT5\n",
    "# duo_pipeline = mono_pipeline % 50 >> duoT5 # apply a rank cutoff of 50 from monoT5 since duoT5 is too costly to run over the full result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e61d4ba3-cd98-4064-a583-2bdac27dfc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = bm25.transform(tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3993f-4cb1-4107-96c7-80412b44e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same but we order based only in the relevance probability of the network\n",
    "\n",
    "def answer(searcher, q, nlp):\n",
    "    data = {\"QueryId\":[], \"EntityId\":[]}\n",
    "    # data = {\"QueryId\":[], \"EntityId\":[], \"Score\":[]}\n",
    "    for i in tqdm(range(len(q))):\n",
    "        qidx  = q[\"QueryId\"][i]\n",
    "        qtext = pre_process(q[\"Query\"][i])\n",
    "        hits = searcher.search(qtext, k = 100)\n",
    "        \n",
    "        h = []\n",
    "        for hit in hits:\n",
    "            # Passing through network\n",
    "            text = qtext+\"\\n\"+ json.loads(hit.raw)[\"contents\"]+\"\\n\"\n",
    "            doc = nlp(text)\n",
    "            # We use partially_relevant here because it is basically (1 - highly_relevant) and we will sort it by increasing order.\n",
    "            h.append([doc.cats['partially_relevant'], str(qidx).zfill(3), hit.docid])\n",
    "            \n",
    "        h = sorted(h)\n",
    "        \n",
    "        # Ordered by confidence on highly_relevant class\n",
    "        for _, qid, eid in h:\n",
    "            data[\"QueryId\"].append(qid)\n",
    "            data[\"EntityId\"].append(eid)\n",
    "        \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4aea2ef-b3c5-4264-adbe-b9a378114478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>text</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>366600</td>\n",
       "      <td>0366601</td>\n",
       "      <td>ancient roman architecturancient roman archite...</td>\n",
       "      <td>0</td>\n",
       "      <td>22.224013</td>\n",
       "      <td>roman architectur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2508825</td>\n",
       "      <td>2508826</td>\n",
       "      <td>list ancient architectur recordlist ancient ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>21.091674</td>\n",
       "      <td>roman architectur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3572539</td>\n",
       "      <td>3572540</td>\n",
       "      <td>roman architectur revolutroman architectur rev...</td>\n",
       "      <td>2</td>\n",
       "      <td>21.075968</td>\n",
       "      <td>roman architectur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1850914</td>\n",
       "      <td>1850915</td>\n",
       "      <td>histori roman byzantin domehistori roman byzan...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.623313</td>\n",
       "      <td>roman architectur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3957053</td>\n",
       "      <td>3957054</td>\n",
       "      <td>sudatoriumarchitectur , sudatorium vault sweat...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.213101</td>\n",
       "      <td>roman architectur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23200</th>\n",
       "      <td>465</td>\n",
       "      <td>3361887</td>\n",
       "      <td>3361888</td>\n",
       "      <td>praemium imperialpraemium imperial ( lit . \\wo...</td>\n",
       "      <td>95</td>\n",
       "      <td>32.834404</td>\n",
       "      <td>organ award nobel prize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23201</th>\n",
       "      <td>465</td>\n",
       "      <td>2350828</td>\n",
       "      <td>2350829</td>\n",
       "      <td>kyoto prize advanc technologkyoto prize advanc...</td>\n",
       "      <td>96</td>\n",
       "      <td>32.804454</td>\n",
       "      <td>organ award nobel prize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23202</th>\n",
       "      <td>465</td>\n",
       "      <td>291347</td>\n",
       "      <td>0291348</td>\n",
       "      <td>albert lasker award basic medic researchalbert...</td>\n",
       "      <td>97</td>\n",
       "      <td>32.784861</td>\n",
       "      <td>organ award nobel prize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23203</th>\n",
       "      <td>465</td>\n",
       "      <td>2946653</td>\n",
       "      <td>2946654</td>\n",
       "      <td>mdecin san frontirmdecin san frontir ( msf ) (...</td>\n",
       "      <td>98</td>\n",
       "      <td>32.777955</td>\n",
       "      <td>organ award nobel prize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23204</th>\n",
       "      <td>465</td>\n",
       "      <td>139459</td>\n",
       "      <td>0139460</td>\n",
       "      <td>2012 nobel peac prize2012 nobel peac prize wa ...</td>\n",
       "      <td>99</td>\n",
       "      <td>32.760371</td>\n",
       "      <td>organ award nobel prize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23205 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid    docid    docno  \\\n",
       "0        2   366600  0366601   \n",
       "1        2  2508825  2508826   \n",
       "2        2  3572539  3572540   \n",
       "3        2  1850914  1850915   \n",
       "4        2  3957053  3957054   \n",
       "...    ...      ...      ...   \n",
       "23200  465  3361887  3361888   \n",
       "23201  465  2350828  2350829   \n",
       "23202  465   291347  0291348   \n",
       "23203  465  2946653  2946654   \n",
       "23204  465   139459  0139460   \n",
       "\n",
       "                                                    text  rank      score  \\\n",
       "0      ancient roman architecturancient roman archite...     0  22.224013   \n",
       "1      list ancient architectur recordlist ancient ar...     1  21.091674   \n",
       "2      roman architectur revolutroman architectur rev...     2  21.075968   \n",
       "3      histori roman byzantin domehistori roman byzan...     3  20.623313   \n",
       "4      sudatoriumarchitectur , sudatorium vault sweat...     4  20.213101   \n",
       "...                                                  ...   ...        ...   \n",
       "23200  praemium imperialpraemium imperial ( lit . \\wo...    95  32.834404   \n",
       "23201  kyoto prize advanc technologkyoto prize advanc...    96  32.804454   \n",
       "23202  albert lasker award basic medic researchalbert...    97  32.784861   \n",
       "23203  mdecin san frontirmdecin san frontir ( msf ) (...    98  32.777955   \n",
       "23204  2012 nobel peac prize2012 nobel peac prize wa ...    99  32.760371   \n",
       "\n",
       "                         query  \n",
       "0            roman architectur  \n",
       "1            roman architectur  \n",
       "2            roman architectur  \n",
       "3            roman architectur  \n",
       "4            roman architectur  \n",
       "...                        ...  \n",
       "23200  organ award nobel prize  \n",
       "23201  organ award nobel prize  \n",
       "23202  organ award nobel prize  \n",
       "23203  organ award nobel prize  \n",
       "23204  organ award nobel prize  \n",
       "\n",
       "[23205 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "833debe5-c919-425f-836c-806e121b16c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:43:35.863 [main] WARN org.terrier.structures.BaseCompressingMetaIndex - Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 943,1 MiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "monoT5 = MonoT5ReRanker(batch_size = 8) # loads castorini/monot5-base-msmarco by default\n",
    "\n",
    "index = pt.IndexFactory.of(\"../data/indexes/pyterrier/\")\n",
    "\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\", metadata=[\"docno\", \"text\"], num_results = 100)\n",
    "mono_pipeline = bm25 >> monoT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8327d81a-2ccf-4210-95e4-04388c3145ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"QueryId\":[], \"EntityId\":[]}\n",
    "qc = {}\n",
    "for i in range(len(ds)):\n",
    "    qid   = str(ds[\"qid\"][i]).zfill(3)\n",
    "    eid   = ds[\"docno\"][i]\n",
    "    \n",
    "    data[\"QueryId\"].append(qid)\n",
    "    data[\"EntityId\"].append(eid)\n",
    "\n",
    "df_answer = pd.DataFrame.from_dict(data)\n",
    "df_answer.to_csv(\"../data/results/pyterrier_t5.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f374bccf-2564-43eb-a636-d87c339e1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answer.to_csv(\"../data/results/19-54.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "488eea49-d655-480c-9c92-92db54220fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|████████████████████████| 2901/2901 [1:05:01<00:00,  1.34s/batches]\n"
     ]
    }
   ],
   "source": [
    "ds = mono_pipeline.transform(tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0aadd8e6-0aa4-410c-82e4-c38c1723c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"QueryId\":[], \"EntityId\":[], \"Rank\":[]}\n",
    "qc = {}\n",
    "for i in range(len(ds)):\n",
    "    qid   = str(ds[\"qid\"][i]).zfill(3)\n",
    "    eid   = ds[\"docno\"][i]\n",
    "    \n",
    "    data[\"QueryId\"].append(qid)\n",
    "    data[\"EntityId\"].append(eid)\n",
    "    data[\"Rank\"].append(ds[\"rank\"][i])\n",
    "\n",
    "df_answer = pd.DataFrame.from_dict(data)\n",
    "# df_answer.to_csv(\"../data/results/pyterrier_t5.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb6a4199-9921-44ca-9049-edf9b8913312",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ddde84f-77e5-4588-bb22-59fb283e1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_answer.sort_values(by=['QueryId', 'Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3be83023-0d86-451a-9cec-57db1a36449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(['Rank'], axis=1).to_csv(\"../data/results/21-09.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a318b295-a0a1-458c-b08c-6ef0c6d5985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>101</td>\n",
       "      <td>2073083</td>\n",
       "      <td>2073084</td>\n",
       "      <td>jean de wavrinjehan ( jean ) de waurin ( wavri...</td>\n",
       "      <td>nobl english person hundr year  war</td>\n",
       "      <td>-0.018614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>101</td>\n",
       "      <td>1896018</td>\n",
       "      <td>1896019</td>\n",
       "      <td>hundr year ' warhundr year ' war wa seri confl...</td>\n",
       "      <td>nobl english person hundr year  war</td>\n",
       "      <td>-0.028138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>101</td>\n",
       "      <td>1896021</td>\n",
       "      <td>1896022</td>\n",
       "      <td>hundr year ' war ( 141553 )lancastrian war wa ...</td>\n",
       "      <td>nobl english person hundr year  war</td>\n",
       "      <td>-0.031801</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>101</td>\n",
       "      <td>4182853</td>\n",
       "      <td>4182854</td>\n",
       "      <td>hundr thirti year ' warhundr thirti year ' war...</td>\n",
       "      <td>nobl english person hundr year  war</td>\n",
       "      <td>-0.038529</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>101</td>\n",
       "      <td>1896019</td>\n",
       "      <td>1896020</td>\n",
       "      <td>hundr year ' war ( 133760 )hundr year ' war , ...</td>\n",
       "      <td>nobl english person hundr year  war</td>\n",
       "      <td>-0.041457</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>97</td>\n",
       "      <td>2609831</td>\n",
       "      <td>2609832</td>\n",
       "      <td>ludila damludila dam graviti dam jinsha river ...</td>\n",
       "      <td>famou river confluenc dam construct</td>\n",
       "      <td>-11.032598</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>97</td>\n",
       "      <td>4337952</td>\n",
       "      <td>4337953</td>\n",
       "      <td>upper atbara setit dam complexupper atbara set...</td>\n",
       "      <td>famou river confluenc dam construct</td>\n",
       "      <td>-11.216054</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>97</td>\n",
       "      <td>1956259</td>\n",
       "      <td>1956260</td>\n",
       "      <td>indu basin projectindu basin project water con...</td>\n",
       "      <td>famou river confluenc dam construct</td>\n",
       "      <td>-12.051404</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>97</td>\n",
       "      <td>4562246</td>\n",
       "      <td>4562247</td>\n",
       "      <td>xiaonanhai damxiaonanhai dam wa propos dam yan...</td>\n",
       "      <td>famou river confluenc dam construct</td>\n",
       "      <td>-12.073783</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>97</td>\n",
       "      <td>3850506</td>\n",
       "      <td>3850507</td>\n",
       "      <td>sondur damsondur dam locat dhamtari district c...</td>\n",
       "      <td>famou river confluenc dam construct</td>\n",
       "      <td>-12.301177</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23205 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid    docid    docno  \\\n",
       "5006  101  2073083  2073084   \n",
       "5047  101  1896018  1896019   \n",
       "5002  101  1896021  1896022   \n",
       "5025  101  4182853  4182854   \n",
       "5068  101  1896019  1896020   \n",
       "...   ...      ...      ...   \n",
       "4957   97  2609831  2609832   \n",
       "4962   97  4337952  4337953   \n",
       "4989   97  1956259  1956260   \n",
       "4982   97  4562246  4562247   \n",
       "4977   97  3850506  3850507   \n",
       "\n",
       "                                                   text  \\\n",
       "5006  jean de wavrinjehan ( jean ) de waurin ( wavri...   \n",
       "5047  hundr year ' warhundr year ' war wa seri confl...   \n",
       "5002  hundr year ' war ( 141553 )lancastrian war wa ...   \n",
       "5025  hundr thirti year ' warhundr thirti year ' war...   \n",
       "5068  hundr year ' war ( 133760 )hundr year ' war , ...   \n",
       "...                                                 ...   \n",
       "4957  ludila damludila dam graviti dam jinsha river ...   \n",
       "4962  upper atbara setit dam complexupper atbara set...   \n",
       "4989  indu basin projectindu basin project water con...   \n",
       "4982  xiaonanhai damxiaonanhai dam wa propos dam yan...   \n",
       "4977  sondur damsondur dam locat dhamtari district c...   \n",
       "\n",
       "                                    query      score  rank  \n",
       "5006  nobl english person hundr year  war  -0.018614     0  \n",
       "5047  nobl english person hundr year  war  -0.028138     1  \n",
       "5002  nobl english person hundr year  war  -0.031801     2  \n",
       "5025  nobl english person hundr year  war  -0.038529     3  \n",
       "5068  nobl english person hundr year  war  -0.041457     4  \n",
       "...                                   ...        ...   ...  \n",
       "4957  famou river confluenc dam construct -11.032598    95  \n",
       "4962  famou river confluenc dam construct -11.216054    96  \n",
       "4989  famou river confluenc dam construct -12.051404    97  \n",
       "4982  famou river confluenc dam construct -12.073783    98  \n",
       "4977  famou river confluenc dam construct -12.301177    99  \n",
       "\n",
       "[23205 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.sort_values(by=['qid', 'rank'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
